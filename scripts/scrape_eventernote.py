#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
from bs4 import BeautifulSoup
import json
import os
import time
import re
from urllib.parse import urljoin, urlparse
import urllib.request
from datetime import datetime

class EventernoteScraper:
    def __init__(self):
        self.base_url = 'https://www.eventernote.com/actors/å°æ—¥å‘ç¾é¦™/63191'
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        self.all_events = []
        self.image_dir = 'scraped_images'
        
        # åˆ›å»ºå›¾ç‰‡ç›®å½•
        if not os.path.exists(self.image_dir):
            os.makedirs(self.image_dir)
    
    def get_page(self, url):
        """è·å–é¡µé¢å†…å®¹"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            return response.text
        except Exception as e:
            print(f"è·å–é¡µé¢å¤±è´¥: {e}")
            return None
    
    def get_total_pages(self, html):
        """è·å–æ€»é¡µæ•°"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯
        text = soup.get_text()
        match = re.search(r'å°æ—¥å‘ç¾é¦™ã®å…¨ã¦ã®ã‚¤ãƒ™ãƒ³ãƒˆã‚’è¦‹ã‚‹\((\d+)ä»¶\)', text)
        if match:
            total_events = int(match.group(1))
            # å‡è®¾æ¯é¡µæ˜¾ç¤º20ä¸ªæ´»åŠ¨
            return max(1, (total_events + 19) // 20)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•æŸ¥æ‰¾åˆ†é¡µé“¾æ¥
        pagination_links = soup.find_all('a', href=re.compile(r'page=\d+'))
        if pagination_links:
            page_numbers = []
            for link in pagination_links:
                match = re.search(r'page=(\d+)', link.get('href', ''))
                if match:
                    page_numbers.append(int(match.group(1)))
            return max(page_numbers) if page_numbers else 1
        
        return 1
    
    def scrape_events_from_page(self, html):
        """ä»é¡µé¢ä¸­æå–æ´»åŠ¨ä¿¡æ¯"""
        soup = BeautifulSoup(html, 'html.parser')
        events = []
        
        # æŸ¥æ‰¾æ´»åŠ¨åˆ—è¡¨é¡¹
        event_items = soup.find_all('li', class_='clearfix')
        
        print(f"æ‰¾åˆ° {len(event_items)} ä¸ªæ´»åŠ¨é¡¹")
        
        for item in event_items:
            event = self.extract_event_info(item)
            if event and event.get('title'):
                events.append(event)
        
        return events
    
    def extract_event_info(self, container):
        """ä»å®¹å™¨ä¸­æå–æ´»åŠ¨ä¿¡æ¯"""
        try:
            # æå–æ—¥æœŸ
            date_text = self.extract_date(container)
            
            # æå–æ ‡é¢˜
            title = self.extract_title(container)
            
            # æå–åœ°ç‚¹
            venue = self.extract_venue(container)
            
            # æå–æ—¶é—´
            time_text = self.extract_time(container)
            
            # æå–å‡ºæ¼”è€…
            performers = self.extract_performers(container)
            
            # æå–å›¾ç‰‡
            image_url = self.extract_image(container)
            
            # è§£ææ—¥æœŸ
            date_info = self.parse_date(date_text)
            
            if not title:
                return None
            
            event = {
                'id': self.generate_id(),
                'day': date_info['day'],
                'dateRange': date_info['dateRange'],
                'fullDate': date_info['fullDate'],
                'year': date_info['year'],
                'month': date_info['month'],
                'day': date_info['day'],
                'title': title,
                'type': self.determine_event_type(title),
                'image': self.download_image(image_url, title),
                'description': f"{title} - å°æ—¥å‘ç¾é¦™å‡ºæ¼”",
                'location': venue,
                'time': self.extract_time_from_text(time_text),
                'openTime': self.extract_open_time(time_text),
                'endTime': self.extract_end_time(time_text),
                'category': self.determine_category(title),
                'priority': self.determine_priority(title),
                'tags': self.generate_tags(title),
                'performers': performers if performers else ['å°æ—¥å‘ç¾é¦™']
            }
            
            return event
            
        except Exception as e:
            print(f"æå–æ´»åŠ¨ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def extract_date(self, container):
        """æå–æ—¥æœŸä¿¡æ¯"""
        # æŸ¥æ‰¾æ—¥æœŸå…ƒç´ 
        date_element = container.find('div', class_='date')
        if date_element:
            day_element = date_element.find('p', class_=re.compile(r'day\d+'))
            if day_element:
                return day_element.get_text(strip=True)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•åœ¨æ–‡æœ¬ä¸­æŸ¥æ‰¾
        text = container.get_text()
        date_match = re.search(r'(\d{4}-\d{2}-\d{2})\s*\(([^)]+)\)', text)
        if date_match:
            return date_match.group(0)
        
        return ""
    
    def extract_title(self, container):
        """æå–æ ‡é¢˜"""
        # æŸ¥æ‰¾äº‹ä»¶å…ƒç´ 
        event_element = container.find('div', class_='event')
        if event_element:
            title_element = event_element.find('h4')
            if title_element:
                return title_element.get_text(strip=True)
        
        return ""
    
    def extract_venue(self, container):
        """æå–åœ°ç‚¹"""
        event_element = container.find('div', class_='event')
        if event_element:
            place_elements = event_element.find_all('div', class_='place')
            for place_element in place_elements:
                text = place_element.get_text()
                if 'ä¼šå ´:' in text:
                    venue_match = re.search(r'ä¼šå ´:\s*([^\n\r]+)', text)
                    if venue_match:
                        return venue_match.group(1).strip()
        
        return ""
    
    def extract_time(self, container):
        """æå–æ—¶é—´ä¿¡æ¯"""
        event_element = container.find('div', class_='event')
        if event_element:
            place_elements = event_element.find_all('div', class_='place')
            for place_element in place_elements:
                text = place_element.get_text()
                if 'é–‹å ´' in text and 'é–‹æ¼”' in text:
                    return text.strip()
        
        return ""
    
    def extract_performers(self, container):
        """æå–å‡ºæ¼”è€…"""
        performers = []
        
        event_element = container.find('div', class_='event')
        if event_element:
            actor_element = event_element.find('div', class_='actor')
            if actor_element:
                performer_links = actor_element.find_all('a')
                for link in performer_links:
                    performer = link.get_text(strip=True)
                    if performer and performer not in performers and performer != 'å‡ºæ¼”è€…:':
                        performers.append(performer)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ï¼Œå°è¯•ä»æ ‡é¢˜ä¸­æå–
        if not performers:
            title = self.extract_title(container)
            if 'MyGO!!!!!' in title:
                performers = ['MyGO!!!!!', 'å°æ—¥å‘ç¾é¦™']
            elif 'ç«‹çŸ³å‡›' in title:
                performers = ['ç«‹çŸ³å‡›', 'å°æ—¥å‘ç¾é¦™']
            else:
                performers = ['å°æ—¥å‘ç¾é¦™']
        
        return performers
    
    def extract_image(self, container):
        """æå–å›¾ç‰‡URL"""
        date_element = container.find('div', class_='date')
        if date_element:
            img_element = date_element.find('img')
            if img_element:
                src = img_element.get('src')
                if src and not src.endswith('logo201607_m.png'):  # æ’é™¤logoå›¾ç‰‡
                    return urljoin(self.base_url, src)
        
        return None
    
    def download_image(self, image_url, title):
        """ä¸‹è½½å›¾ç‰‡å¹¶è¿”å›æœ¬åœ°è·¯å¾„"""
        if not image_url:
            # å¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œä½¿ç”¨é»˜è®¤å›¾ç‰‡
            return "image/randpic_å°æ—¥å‘ç¾é¦™_1.jpg"
        
        try:
            # ç”Ÿæˆæ–‡ä»¶å
            safe_title = re.sub(r'[^\w\s-]', '', title)[:50]
            file_extension = os.path.splitext(urlparse(image_url).path)[1] or '.jpg'
            filename = f"{safe_title}_{int(time.time())}{file_extension}"
            filepath = os.path.join(self.image_dir, filename)
            
            # ä¸‹è½½å›¾ç‰‡
            response = self.session.get(image_url, timeout=10)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            print(f"ä¸‹è½½å›¾ç‰‡: {filename}")
            return filepath
            
        except Exception as e:
            print(f"ä¸‹è½½å›¾ç‰‡å¤±è´¥: {e}")
            return "image/randpic_å°æ—¥å‘ç¾é¦™_1.jpg"
    
    def parse_date(self, date_text):
        """è§£ææ—¥æœŸ"""
        # åŒ¹é…æ ¼å¼: "2025-07-30 (æ°´)"
        match = re.search(r'(\d{4})-(\d{2})-(\d{2})\s*\(([^)]+)\)', date_text)
        if match:
            year = int(match.group(1))
            month = int(match.group(2))
            day = int(match.group(3))
            weekday = match.group(4)
            
            return {
                'year': year,
                'month': month,
                'day': day,
                'dateRange': f"{month}.{day:02d}",
                'fullDate': f"{year}-{match.group(2)}-{match.group(3)}",
                'day': self.get_weekday_abbr(weekday)
            }
        
        # é»˜è®¤è¿”å›
        return {
            'year': 2025,
            'month': 1,
            'day': 1,
            'dateRange': "01.01",
            'fullDate': "2025-01-01",
            'day': "Mon."
        }
    
    def get_weekday_abbr(self, weekday):
        """è·å–æ˜ŸæœŸç¼©å†™"""
        weekday_map = {
            'æœˆ': 'Mon.',
            'ç«': 'Tue.',
            'æ°´': 'Wed.',
            'æœ¨': 'Thu.',
            'é‡‘': 'Fri.',
            'åœŸ': 'Sat.',
            'æ—¥': 'Sun.'
        }
        return weekday_map.get(weekday, 'Mon.')
    
    def determine_event_type(self, title):
        """ç¡®å®šæ´»åŠ¨ç±»å‹"""
        title_lower = title.lower()
        if any(keyword in title_lower for keyword in ['ãƒ©ã‚¤ãƒ–', 'live', 'concert']):
            return 'concert'
        elif any(keyword in title_lower for keyword in ['èˆå°', 'theater']):
            return 'theater'
        elif any(keyword in title_lower for keyword in ['ã‚¤ãƒ™ãƒ³ãƒˆ', 'event']):
            return 'event'
        else:
            return 'event'
    
    def determine_category(self, title):
        """ç¡®å®šæ´»åŠ¨åˆ†ç±»"""
        title_lower = title.lower()
        if any(keyword in title_lower for keyword in ['ãƒ©ã‚¤ãƒ–', 'live', 'concert', 'èˆå°', 'theater']):
            return 'performance'
        else:
            return 'event'
    
    def determine_priority(self, title):
        """ç¡®å®šä¼˜å…ˆçº§"""
        title_lower = title.lower()
        if any(keyword in title_lower for keyword in ['ãƒ©ã‚¤ãƒ–', 'live', 'concert']):
            return 3
        elif any(keyword in title_lower for keyword in ['èˆå°', 'theater']):
            return 2
        else:
            return 1
    
    def generate_tags(self, title):
        """ç”Ÿæˆæ ‡ç­¾"""
        tags = []
        title_lower = title.lower()
        
        if 'mygo' in title_lower:
            tags.append('MyGO!!!!!')
        if any(keyword in title_lower for keyword in ['ãƒ©ã‚¤ãƒ–', 'live']):
            tags.append('æ¼”å”±ä¼š')
        if 'èˆå°' in title_lower:
            tags.append('èˆå°å‰§')
        if any(keyword in title_lower for keyword in ['ã‚¤ãƒ™ãƒ³ãƒˆ', 'event']):
            tags.append('æ´»åŠ¨')
        
        return tags
    
    def extract_time_from_text(self, time_text):
        """ä»æ—¶é—´æ–‡æœ¬ä¸­æå–å¼€æ¼”æ—¶é—´"""
        match = re.search(r'é–‹æ¼”\s*(\d{1,2}:\d{2})', time_text)
        return match.group(1) if match else ""
    
    def extract_open_time(self, time_text):
        """ä»æ—¶é—´æ–‡æœ¬ä¸­æå–å¼€åœºæ—¶é—´"""
        match = re.search(r'é–‹å ´\s*(\d{1,2}:\d{2})', time_text)
        return match.group(1) if match else ""
    
    def extract_end_time(self, time_text):
        """ä»æ—¶é—´æ–‡æœ¬ä¸­æå–ç»“æŸæ—¶é—´"""
        match = re.search(r'çµ‚æ¼”\s*(\d{1,2}:\d{2})', time_text)
        return match.group(1) if match else ""
    
    def generate_id(self):
        """ç”Ÿæˆå”¯ä¸€ID"""
        return int(time.time() * 1000) + hash(str(time.time()))
    
    def scrape_all_events(self):
        """çˆ¬å–æ‰€æœ‰æ´»åŠ¨"""
        print("å¼€å§‹çˆ¬å–å°æ—¥å‘ç¾é¦™çš„æ´»åŠ¨æ•°æ®...")
        
        # è·å–æ€»é¡µæ•°ï¼ˆä»ç¬¬ä¸€é¡µè·å–ï¼‰
        first_page_url = f"{self.base_url}/events?actor_id=63191&limit=20&page=1"
        html = self.get_page(first_page_url)
        if not html:
            print("æ— æ³•è·å–é¡µé¢å†…å®¹")
            return
        
        # è·å–æ€»é¡µæ•°
        total_pages = self.get_total_pages(html)
        print(f"æ£€æµ‹åˆ°æ€»å…±æœ‰ {total_pages} é¡µæ•°æ®")
        
        # çˆ¬å–æ‰€æœ‰é¡µé¢
        for page in range(1, total_pages + 1):
            print(f"æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...")
            
            # ä½¿ç”¨æ­£ç¡®çš„åˆ†é¡µURLæ ¼å¼
            page_url = f"{self.base_url}/events?actor_id=63191&limit=20&page={page}"
            html = self.get_page(page_url)
            
            if html:
                events = self.scrape_events_from_page(html)
                # å»é‡å¤„ç†
                new_events = self.remove_duplicates(events)
                self.all_events.extend(new_events)
                print(f"ç¬¬ {page} é¡µå®Œæˆï¼Œè·å–åˆ° {len(events)} ä¸ªæ´»åŠ¨ï¼Œå»é‡å {len(new_events)} ä¸ª")
            else:
                print(f"ç¬¬ {page} é¡µè·å–å¤±è´¥")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«åçˆ¬
            time.sleep(2)
        
        print(f"çˆ¬å–å®Œæˆï¼æ€»å…±è·å–åˆ° {len(self.all_events)} ä¸ªæ´»åŠ¨")
        
        # ä¿å­˜æ•°æ®
        self.save_data()
    
    def remove_duplicates(self, new_events):
        """å»é™¤é‡å¤çš„æ´»åŠ¨"""
        existing_titles = set()
        for event in self.all_events:
            existing_titles.add(event['title'])
        
        unique_events = []
        for event in new_events:
            if event['title'] not in existing_titles:
                unique_events.append(event)
        
        return unique_events
    
    def save_data(self):
        """ä¿å­˜æ•°æ®åˆ°JSONæ–‡ä»¶"""
        data = {
            'events': self.all_events,
            'featuredPerson': {
                'name': "å°æ—¥å‘ç¾é¦™",
                'japaneseName': "ã“ã²ãªãŸã¿ã‹",
                'description': "å£°å„ªã€ã‚¢ã‚¤ãƒ‰ãƒ«ã€ã‚¢ãƒ¼ãƒ†ã‚£ã‚¹ãƒˆ",
                'image': "image/randpic_å°æ—¥å‘ç¾é¦™_1.jpg"
            },
            'categories': [
                {
                    'id': "performance",
                    'name': "æ¼”å‡ºæ´»åŠ¨",
                    'icon': "ğŸ­"
                },
                {
                    'id': "event",
                    'name': "å…¶ä»–æ´»åŠ¨",
                    'icon': "ğŸª"
                }
            ]
        }
        
        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        with open('scraped_activities.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print("æ•°æ®å·²ä¿å­˜åˆ° scraped_activities.json")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        self.generate_report()
    
    def generate_report(self):
        """ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š"""
        report = {
            'totalEvents': len(self.all_events),
            'eventsByMonth': {},
            'eventsByType': {},
            'eventsByCategory': {}
        }
        
        for event in self.all_events:
            # æŒ‰æœˆä»½ç»Ÿè®¡
            month = event['month']
            report['eventsByMonth'][month] = report['eventsByMonth'].get(month, 0) + 1
            
            # æŒ‰ç±»å‹ç»Ÿè®¡
            event_type = event['type']
            report['eventsByType'][event_type] = report['eventsByType'].get(event_type, 0) + 1
            
            # æŒ‰åˆ†ç±»ç»Ÿè®¡
            category = event['category']
            report['eventsByCategory'][category] = report['eventsByCategory'].get(category, 0) + 1
        
        with open('scraping_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print("ç»Ÿè®¡æŠ¥å‘Šå·²ä¿å­˜åˆ° scraping_report.json")
        print(f"æ€»æ´»åŠ¨æ•°: {report['totalEvents']}")
        print(f"æŒ‰æœˆä»½åˆ†å¸ƒ: {report['eventsByMonth']}")
        print(f"æŒ‰ç±»å‹åˆ†å¸ƒ: {report['eventsByType']}")
        print(f"æŒ‰åˆ†ç±»åˆ†å¸ƒ: {report['eventsByCategory']}")

def main():
    scraper = EventernoteScraper()
    scraper.scrape_all_events()

if __name__ == "__main__":
    main() 